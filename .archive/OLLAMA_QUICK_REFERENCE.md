# Ollama Quick Reference

## ğŸš€ Common Commands

### Start/Stop Services

```bash
# Start Ollama server (if not running as service)
ollama serve

# Check if Ollama is running
curl http://localhost:11434/api/tags

# List all installed models
ollama list

# Download a new model
ollama pull mistral:7b-instruct-v0.3-q4_K_M
ollama pull llama2:latest
ollama pull llama3.2:3b-instruct-q4_K_M
```

### Run Application

```bash
# Activate virtual environment
cd J:\Company\amasQIS\AI\NeuroSan\Neurosan-v02
.\venv\Scripts\Activate.ps1

# Run with default settings (uses Ollama)
py -m run

# View logs
Get-Content logs/thinking_dir/* -Wait
```

---

## ğŸ¯ Model Quick Selection

### For First Time (Fastest Setup)
```dotenv
LLM_PROVIDER=ollama
OLLAMA_MODEL=mistral:7b-instruct-v0.3-q4_K_M
OLLAMA_BASE_URL=http://localhost:11434
```

### For Lightweight/Speed
```dotenv
OLLAMA_MODEL=llama3.2:3b-instruct-q4_K_M
```

### For Quality
```dotenv
OLLAMA_MODEL=llama2:latest
```

### For Cloud Power
```dotenv
OLLAMA_MODEL=qwen3-next:80b-cloud
OLLAMA_CLOUD_API_KEY=your_api_key
```

---

## ğŸ“Š Model Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model                â”‚ Size   â”‚ Speed   â”‚ Quality â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llama3.2:3b          â”‚ 2.0 GB â”‚ âš¡âš¡âš¡   â”‚ â­â­   â”‚
â”‚ mistral:7b*          â”‚ 4.4 GB â”‚ âš¡âš¡    â”‚ â­â­â­ â”‚
â”‚ llama2:latest        â”‚ 3.8 GB â”‚ âš¡âš¡    â”‚ â­â­â­ â”‚
â”‚ qwen3-next:80b*      â”‚ Cloud  â”‚ âš¡     â”‚ â­â­â­â­â”‚
â”‚ mistral-large:675b*  â”‚ Cloud  â”‚ âš¡     â”‚ â­â­â­â­â”‚
â”‚ deepseek-v3.1:671b*  â”‚ Cloud  â”‚ âš¡     â”‚ â­â­â­â­â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* = Our current model / Cloud model requires API key
```

---

## ğŸ”„ Switching Models

### Quick Switch Steps

1. **Edit `.env`:**
   ```bash
   # Change this line:
   OLLAMA_MODEL=mistral:7b-instruct-v0.3-q4_K_M
   
   # To this (example):
   OLLAMA_MODEL=llama2:latest
   ```

2. **Restart application:**
   ```bash
   py -m run
   ```

3. **Done!** New model is active

---

## ğŸ§ª Test Your Setup

### Minimal Test
```bash
# Check Ollama connection
curl http://localhost:11434/api/tags

# Should show:
# {
#   "models": [
#     {"name": "mistral:7b-instruct-v0.3-q4_K_M", ...},
#     ...
#   ]
# }
```

### Full Application Test
```bash
py -m run

# Check logs - should see:
# âœ“ Environment variables loaded
# âœ“ LLM provider configured as: ollama
# âœ“ Model: mistral:7b-instruct-v0.3-q4_K_M
# âœ“ Server starting...
```

---

## ğŸ“ .env Configuration Quick Map

```
.env File Location:
J:\Company\amasQIS\AI\NeuroSan\Neurosan-v02\.env

Key Sections:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Provider Selection              â”‚
â”‚ LLM_PROVIDER=ollama                 â”‚ â† Change here
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Configuration                â”‚
â”‚ OLLAMA_MODEL=mistral:7b...          â”‚ â† Change model here
â”‚ OLLAMA_BASE_URL=http://localhost... â”‚ â† Change server here
â”‚ OLLAMA_TEMPERATURE=0.7              â”‚ â† Tune parameters here
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ Common Issues & Quick Fixes

| Issue | Quick Fix |
|-------|-----------|
| "Connection refused" | Run `ollama serve` |
| "Model not found" | Run `ollama pull mistral:7b-instruct-v0.3-q4_K_M` |
| "Out of memory" | Use smaller model (3B instead of 7B) |
| "Slow responses" | Use faster model or cloud option |
| "App not using new model" | Restart with `py -m run` |

---

## ğŸ“ˆ Performance Tuning

### Fast Response (Trade quality for speed)
```dotenv
OLLAMA_MODEL=llama3.2:3b-instruct-q4_K_M
OLLAMA_TEMPERATURE=0.5
OLLAMA_MAX_TOKENS=512
OLLAMA_TIMEOUT=60
```

### Best Quality (Accept slower responses)
```dotenv
OLLAMA_MODEL=llama2:latest
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2048
OLLAMA_TIMEOUT=300
```

### Balanced (Recommended)
```dotenv
OLLAMA_MODEL=mistral:7b-instruct-v0.3-q4_K_M
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=1024
OLLAMA_TIMEOUT=180
```

---

## ğŸ”— Useful Links

| Resource | URL |
|----------|-----|
| Ollama Official | https://ollama.ai |
| Ollama Models | https://ollama.ai/library |
| Ollama Docs | https://github.com/ollama/ollama |
| Neuro-SAN Docs | See `docs/` folder |
| LangChain Ollama | https://python.langchain.com/docs/integrations/llms/ollama |

---

## ğŸ“ Configuration Examples

### Example 1: Fast & Free (Default)
```dotenv
LLM_PROVIDER=ollama
OLLAMA_MODEL=mistral:7b-instruct-v0.3-q4_K_M
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TEMPERATURE=0.7
```

### Example 2: Ultra Fast (Lightweight)
```dotenv
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3.2:3b-instruct-q4_K_M
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TEMPERATURE=0.5
OLLAMA_MAX_TOKENS=512
```

### Example 3: High Quality
```dotenv
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama2:latest
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TEMPERATURE=0.8
OLLAMA_MAX_TOKENS=2048
```

### Example 4: Cloud Powered (Powerful)
```dotenv
LLM_PROVIDER=ollama
OLLAMA_MODEL=qwen3-next:80b-cloud
OLLAMA_CLOUD_API_KEY=your_key_here
OLLAMA_TIMEOUT=300
```

---

## ğŸ¯ Which Model Should I Use?

### Decision Tree

```
Start here â†’ Need to download model?
             â”œâ”€ YES â†’ Use local (Free, no key needed)
             â”‚        â””â”€ Need fast? â†’ llama3.2:3b (2GB)
             â”‚        â””â”€ Need quality? â†’ mistral:7b (4.4GB)
             â”‚        â””â”€ Need best local? â†’ llama2:latest
             â”‚
             â””â”€ NO â†’ Use cloud (API key needed, faster)
                     â””â”€ General â†’ qwen3-next:80b-cloud
                     â””â”€ Coding â†’ qwen3-coder:480b-cloud
                     â””â”€ Latest â†’ deepseek-v3.1:671b-cloud
```

---

## âœ… Verification Checklist

- [ ] Ollama installed and running (`ollama serve`)
- [ ] At least one model downloaded (`ollama list`)
- [ ] `.env` file configured with LLM settings
- [ ] Virtual environment activated
- [ ] Application started with `py -m run`
- [ ] No errors in logs
- [ ] Can see model responses

---

## ğŸ†˜ Getting Help

**Check these in order:**

1. **Verify Ollama:**
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. **Check installed models:**
   ```bash
   ollama list
   ```

3. **Check app logs:**
   ```bash
   Get-Content logs/thinking_dir/* -Tail 20
   ```

4. **Restart everything:**
   ```bash
   ollama serve  # In separate window
   py -m run      # In another window
   ```

---

## ğŸ“š Additional Resources

- Full setup guide: `OLLAMA_SETUP_GUIDE.md`
- Migration details: `OLLAMA_MIGRATION_PLAN.md`
- Original README: `README.md`
- User guide: `docs/user_guide.md`

---

**Last Updated:** December 21, 2025
