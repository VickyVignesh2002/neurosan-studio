# Copyright Â© 2025 Cognizant Technology Solutions Corp, www.cognizant.com.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# END COPYRIGHT

# ============================================================================
# Shared LLM Configuration for Neuro-SAN Agent Networks
# ============================================================================
#
# This file provides centralized LLM configuration that can be included by
# all agent network HOCON files. Change the model here to switch all agents.
#
# USAGE: In your agent network HOCON file, add:
#   include "registries/llm_config.hocon"
#   ...
#   "llm_config": ${shared_llm_config},
#
# For complex agents requiring more capable models:
#   "llm_config": ${shared_llm_config_advanced},
#
# ============================================================================

{
    # -------------------------------------------------------------------------
    # Standard LLM Config - For most agent networks
    # -------------------------------------------------------------------------
    # Uses Ollama local model for fast, free inference
    # Default: mistral:7b-instruct-v0.3-q4_K_M (4.4GB, fast)
    #
    # Override via environment variable: MODEL_NAME
    # Example: set MODEL_NAME=llama3.2:3b-instruct-q4_K_M
    #
    "shared_llm_config": {
        "class": "ollama",
        "model_name": "mistral:7b-instruct-v0.3-q4_K_M",
        "model_name": ${?MODEL_NAME}
    },

    # -------------------------------------------------------------------------
    # Advanced LLM Config - For complex agent networks
    # -------------------------------------------------------------------------
    # Uses Ollama cloud models for tasks requiring higher capability
    # Default: deepseek-v3.1:671b-cloud (latest, most capable)
    #
    # Available Ollama Cloud Models (require OLLAMA_CLOUD_API_KEY):
    #   - deepseek-v3.1:671b-cloud (latest, most capable)
    #   - mistral-large-3:675b-cloud (multilingual, reasoning)
    #   - qwen3-coder:480b-cloud (strong for coding)
    #   - kimi-k2:1t-cloud (massive model)
    #
    # Override via environment variable: MODEL_NAME_ADVANCED
    #
    "shared_llm_config_advanced": {
        "class": "ollama",
        "model_name": "deepseek-v3.1:671b-cloud",
        "model_name": ${?MODEL_NAME_ADVANCED}
    },

    # -------------------------------------------------------------------------
    # Local-Only LLM Config - For offline use
    # -------------------------------------------------------------------------
    # Guaranteed to work without internet/API keys
    # Uses smaller, faster local models
    #
    "shared_llm_config_local": {
        "class": "ollama",
        "model_name": "mistral:7b-instruct-v0.3-q4_K_M",
        "model_name": ${?MODEL_NAME_LOCAL}
    }
}